{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e82bf3",
   "metadata": {},
   "source": [
    "## This notebook is meant to accompany the trial_nondetection_MAXI.py file that is included in this directory with additional comments. \n",
    "\n",
    "### This analysis can produce $\\sim 330$ GB of data so be sure that there is enough storage on your computer.\n",
    "\n",
    "In this notebook, we will go through the code to produce Figure 6 of the associated BAT survey paper. This example outlines how to analyze BAT survey data to obtain a light curve/flux upper limits for a newly identified source, such as MAXI J0637-430. \n",
    "\n",
    "First, we need to import our usual python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import batanalysis as ba\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.io import fits\n",
    "from pathlib import Path\n",
    "import swiftbat\n",
    "import pickle\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3271e6",
   "metadata": {},
   "source": [
    "Then we need to create a custom catalog file with the MAXI source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c554d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_name='MAXI J0637-430'\n",
    "\n",
    "#define the coordinates in RA/Dec & galactic Lat/lon\n",
    "object_ra=99.09830\n",
    "object_dec=-42.86781\n",
    "object_glat=251.51841\n",
    "object_glon=-20.67087\n",
    "\n",
    "incat=ba.create_custom_catalog(object_name, object_ra, object_dec, object_lat, object_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9a0c31",
   "metadata": {},
   "source": [
    "If we were continuing an analysis and the above cell was already run, we do not need to create the custom catalog. Instead we can simply do:\n",
    "```\n",
    "incat=Path(\"path/to/custom_catalog.cat\")\n",
    "```\n",
    "\n",
    "Now that we have our catalog of sources with the recently identified MAXI source, we can search for all the BAT survey datasets that have this object in the BAT FOV with a partial coding fraction of ~19% (ie the area of the detector plane that is exposed to the source coordinates is $> 1000$ cm$^2$.\n",
    "\n",
    "We query HEASARC for all observations from 2019-11-01 to 2020-01-30, which is when the source was first found and undergoing spectral transitions. Then, we filter these observations to determine which meet our `minexposure` criteria and download them into the current working directory. When then obtain the observation IDs for the successfully downloaded datasets and exclude some problematic observation IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5267ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryargs = dict(time=\"2019-11-01 .. 2020-01-30\", fields='All', resultmax=0)\n",
    "object_batsource = swiftbat.source(ra=object_ra, dec=object_dec, name=object_name)\n",
    "table_everything = ba.from_heasarc(**queryargs)\n",
    "\n",
    "minexposure = 1000     # cm^2 after cos adjust\n",
    "exposures = np.array([object_batsource.exposure(ra=row['RA'], dec=row['DEC'], roll=row['ROLL_ANGLE'])[0] for row in table_everything])\n",
    "table_exposed = table_everything[exposures > minexposure]\n",
    "\n",
    "result = ba.download_swiftdata(table_exposed)\n",
    "\n",
    "obs_ids=[i for i in table_exposed['OBSID'] if result[i]['success'] and i not in [\"00012012026\", \"00012172020\", \"00035344062\", \"00045604023\", \"00095400024\", \"03102102001\", \"03109915005\", \"03110367008\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402e75fd",
   "metadata": {},
   "source": [
    "If the user is continuing an analysis, they do not need to do the whole querying/downloading part of the workflow again. They can simply loop through the observation IDs that have already been downloaded and analyzed by doing:\n",
    "```\n",
    "obs_ids=[i.parent.name.split(\"_\")[0] for i in sorted(ba.datadir().glob(\"*_surveyresult/batsurvey.pickle\"))]\n",
    "```\n",
    "\n",
    "\n",
    "With the data downloaded, we can now craft our dictionary of `batsurvey` key/value pairs that we want passed to the HEASoft function, then set the path to where the pattern maps are located and then call the parallelized analysis function. \n",
    "\n",
    "In setting our `batsurvey` key/value pairs we denote that the detector thresholds should be such that the number of BAT active detectors are $>8000$. This value was obtained by analyzing the observations with `detthresh` and `detthresh2` set to 9000, which is the default value, and looking through the failure messages that were saved to each BatSurvey object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict=dict(cleansnr=6,cleanexpr='ALWAYS_CLEAN==T', incatalog=f\"{incat}\", detthresh=8000, detthresh2=8000)\n",
    "noise_map_dir=Path(\"/local/data/tparsota/PATTERN_MAPS/\")\n",
    "batsurvey_obs=ba.parallel.batsurvey_analysis(obs_ids, input_dict=input_dict, patt_noise_dir=noise_map_dir, nprocs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1091e",
   "metadata": {},
   "source": [
    "Similar to our other analyses, we can now calculate the spectrum for each pointing, the detector response function, and subsequently fit the spectra with the default `cflux*po` model in Xspec. Here, we set `use_cstat=True` since we expect our MAXI source to have low enough counts such that xspec needs to take poisson statistics into account (this is set to be True by default but we are explicit here for the user's knowledge). We also explicitly set `ul_pl_index=2` which sets the photon index of the power law used to obtain a flux upper limit to be 2, which is the default value but we specify it explicitly here for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batsurvey_obs=ba.parallel.batspectrum_analysis(batsurvey_obs, object_name, use_cstat=True, ul_pl_index=2, nprocs=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c6483",
   "metadata": {},
   "source": [
    "Next, we can create our outventory file and define our time bins for mosaicing the BAT survey data. Notice that we pass our custom inventory file into the `batmosaic_analysis` for it to be able to search for the new MAXI source that we are interested in.\n",
    "\n",
    "Remember that when continuing an analysis, the outventory file can just be set to be the path to the previously created outventory file (similar to defining the custom catalog when continuing a simulation).\n",
    "\n",
    "We only use `nproc=3` here since each process uses ~10 GB and our laptop has limited memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fd383",
   "metadata": {},
   "outputs": [],
   "source": [
    "outventory_file=ba.merge_outventory(batsurvey_obs)\n",
    "time_bins=ba.group_outventory(outventory_file, np.timedelta64(1, \"W\"))\n",
    "mosaic_list, total_mosaic=ba.parallel.batmosaic_analysis(batsurvey_obs, outventory_file, time_bins, catalog_file=incat, nprocs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083fc72",
   "metadata": {},
   "source": [
    "Now we can conduct our spectral analyses for the source of interest in the weekly mosaics and the total 2 month mosaic image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_list=ba.parallel.batspectrum_analysis(mosaic_list, object_name, use_cstat=True, nprocs=5)\n",
    "total_mosaic=ba.parallel.batspectrum_analysis(total_mosaic, object_name, use_cstat=True, nprocs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc487777",
   "metadata": {},
   "source": [
    "We can also now take a look at the results of the survey analyses and the mosaic analyses (although we could have looked at the results of the survey analyses earlier to get some insight into the analyses and any issues that may have popped up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes=ba.plot_survey_lc([batsurvey_obs,mosaic_list], id_list= object_name, time_unit=\"UTC\", values=[\"rate\",\"snr\", \"flux\", \"PhoIndex\", \"exposure\"], same_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42336b91",
   "metadata": {},
   "source": [
    "The next cell concatenates information from the BAT survey analyses and the mosaic analyses for us to plot in publication quality figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data in a dictionary for convenient custom plotting for publication quality figures\n",
    "all_data=ba.concatenate_data(batsurvey_obs, object_name, [\"met_time\", \"utc_time\", \"exposure\", \"rate\",\"rate_err\",\"snr\", \"flux\", \"PhoIndex\"])\n",
    "\n",
    "with open('all_data_dictionary.pkl', 'wb') as f:\n",
    "     pickle.dump(all_data, f)\n",
    "     \n",
    "all_data_weekly=ba.concatenate_data(mosaic_list, object_name, [\"user_timebin/met_time\", \"user_timebin/met_stop_time\", \"user_timebin/utc_time\", \"user_timebin/utc_stop_time\", \"exposure\", \"rate\",\"rate_err\",\"snr\", \"flux\", \"PhoIndex\"])\n",
    "\n",
    "with open('weekly_mosaic_dictionary.pkl', 'wb') as f:\n",
    "     pickle.dump(all_data_weekly, f)\n",
    "\n",
    "#make the plot\n",
    "energy_range=None\n",
    "time_unit=\"MET\"\n",
    "values=[\"rate\", \"snr\", \"flux\"]\n",
    "\n",
    "survey_obsid_list=[\"all_data_dictionary\", \"weekly_mosaic_dictionary\"]\n",
    "\n",
    "obs_list_count=0\n",
    "for observation_list in survey_obsid_list:\n",
    "\n",
    "    with open(observation_list+\".pkl\", 'rb') as f:\n",
    "        all_data=pickle.load(f)\n",
    "        data=all_data[object_name]\n",
    "\n",
    "    # get the time centers and errors\n",
    "    if \"mosaic\" in observation_list:\n",
    "\n",
    "        if \"MET\" in time_unit:\n",
    "            t0 = TimeDelta(data[\"user_timebin/met_time\"], format='sec')\n",
    "            tf = TimeDelta(data[\"user_timebin/met_stop_time\"], format='sec')\n",
    "        elif \"MJD\" in time_unit:\n",
    "            t0 = Time(data[time_str_start], format='mjd')\n",
    "            tf = Time(data[time_str_end], format='mjd')\n",
    "        else:\n",
    "            t0 = Time(data[\"user_timebin/utc_time\"])\n",
    "            tf = Time(data[\"user_timebin/utc_stop_time\"])\n",
    "    else:\n",
    "        if \"MET\" in time_unit:\n",
    "            t0 = TimeDelta(data[\"met_time\"], format='sec')\n",
    "        elif \"MJD\" in time_unit:\n",
    "            t0 = Time(data[time_str_start], format='mjd')\n",
    "        else:\n",
    "            t0 = Time(data[\"utc_time\"])\n",
    "        tf = t0 + TimeDelta(data[\"exposure\"], format='sec')\n",
    "\n",
    "    dt = tf - t0\n",
    "\n",
    "    if \"MET\" in time_unit:\n",
    "        time_center = 0.5 * (tf + t0).value\n",
    "        time_diff = 0.5 * (tf - t0).value\n",
    "    elif \"MJD\" in time_unit:\n",
    "        time_diff = 0.5 * (tf - t0)\n",
    "        time_center = t0 + time_diff\n",
    "        time_center = time_center.value\n",
    "        time_diff = time_diff.value\n",
    "\n",
    "    else:\n",
    "        time_diff = TimeDelta(0.5 * dt)  # dt.to_value('datetime')\n",
    "        time_center = t0 + time_diff\n",
    "\n",
    "        time_center = np.array([i.to_value('datetime64') for i in time_center])\n",
    "        time_diff = np.array([np.timedelta64(0.5 * i.to_datetime()) for i in dt])\n",
    "\n",
    "    x = time_center\n",
    "    xerr = time_diff\n",
    "\n",
    "    if obs_list_count == 0:\n",
    "        fig, axes = plt.subplots(len(values), sharex=True) #, figsize=(10,12))\n",
    "\n",
    "    axes_queue = [i for i in range(len(values))]\n",
    "    # plot_value=[i for i in values]\n",
    "\n",
    "    e_range_str = f\"{14}-{195} keV\"\n",
    "    #axes[0].set_title(object_name + '; survey data from ' + e_range_str)\n",
    "\n",
    "    for i in values:\n",
    "        ax = axes[axes_queue[0]]\n",
    "        axes_queue.pop(0)\n",
    "\n",
    "        y = data[i]\n",
    "        yerr = np.zeros(x.size)\n",
    "        y_upperlim = np.zeros(x.size)\n",
    "\n",
    "        label = i\n",
    "\n",
    "        if \"rate\" in i:\n",
    "            yerr = data[i + \"_err\"]\n",
    "            label = \"Count rate (cts/s)\"\n",
    "        elif i + \"_lolim\" in data.keys():\n",
    "            # get the errors\n",
    "            lolim = data[i + \"_lolim\"]\n",
    "            hilim = data[i + \"_hilim\"]\n",
    "\n",
    "            yerr = np.array([lolim, hilim])\n",
    "            y_upperlim = data[i + \"_upperlim\"]\n",
    "\n",
    "            # find where we have upper limits and set the error to 1 since the nan error value isnt\n",
    "            # compatible with upperlimits\n",
    "            yerr[:, y_upperlim] = 0.2 * y[y_upperlim]\n",
    "\n",
    "        if \"mosaic\" in observation_list:\n",
    "            if \"weekly\" in observation_list:\n",
    "                zorder = 9\n",
    "                c = \"blue\"\n",
    "                m = \"o\"\n",
    "                l=\"Weekly Mosaic\"\n",
    "                ms=5\n",
    "                a=0.8\n",
    "            else:\n",
    "                zorder = 9\n",
    "                c='green'\n",
    "                m = \"s\"\n",
    "                l = \"Monthly Mosaic\"\n",
    "                ms=7\n",
    "                a = 1\n",
    "        else:\n",
    "            zorder = 4\n",
    "            c = \"gray\"\n",
    "            m = \".\"\n",
    "            l = \"Survey Snapshot\"\n",
    "            ms=3\n",
    "            a = 0.3\n",
    "\n",
    "        ax.errorbar(x, y, xerr=xerr, yerr=yerr, uplims=y_upperlim, linestyle=\"None\", marker=m, markersize=ms,\n",
    "                    zorder=zorder, color=c, label=l, alpha=a)\n",
    "                    \n",
    "        #plt.gca().ticklabel_format(useMathText=True)\n",
    "        ax.xaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))\n",
    "\n",
    "\n",
    "        if (\"flux\" in i.lower()):\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        if (\"snr\" in i.lower()):\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        ax.set_ylabel(label)\n",
    "\n",
    "    # if T0==0:\n",
    "    if \"MET\" in time_unit:\n",
    "        label_string = 'MET Time (s)'\n",
    "    elif \"MJD\" in time_unit:\n",
    "        label_string = 'MJD Time (s)'\n",
    "    else:\n",
    "        label_string = 'UTC Time (s)'\n",
    "\n",
    "    axes[-1].set_xlabel(label_string)\n",
    "    \n",
    "    obs_list_count += 1\n",
    "\n",
    "\n",
    "#add the UTC times as well\n",
    "utc_time=Time([\"2019-11-01\", \"2019-12-01\", \"2020-01-01\", \"2020-01-30\"])\n",
    "met_time=[]\n",
    "for i in utc_time:\n",
    "    met_time.append(sbu.datetime2met(i.datetime, correct=True))\n",
    "\n",
    "for i,j in zip(met_time, utc_time.ymdhms):\n",
    "    for ax in axes:\n",
    "        ax.axvline(i, 0, 1, ls='--', color='k')\n",
    "        if ax==axes[0]:\n",
    "            ax.text(i, ax.get_ylim()[1]*1.03, f'{j[\"year\"]}-{j[\"month\"]}-{j[\"day\"]}', fontsize=10, ha='center')\n",
    "\n",
    "axes[1].set_ylabel(\"SNR\")\n",
    "axes[2].set_ylabel(r\"Flux (erg/s/cm$^2$)\")\n",
    "\n",
    "axes[1].legend(loc= \"lower center\", ncol=2)\n",
    "\n",
    "for ax, l in zip(axes, [\"a\",\"b\",\"c\",\"d\"]):\n",
    "    ax.text(1.0, .95, f\"({l})\", ha='right', va='top', transform=ax.transAxes,  fontsize=12)\n",
    "\n",
    "fig.tight_layout()\n",
    "plot_filename = object_name + '_survey_lc.pdf'\n",
    "fig.savefig(plot_filename, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
